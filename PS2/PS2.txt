EECS-349 PS2Srikanth Kilaru1. “Alcohol” is the best attribute as it has the highest level of separation between the two class histograms2. Accuracy is 62.381%. ZeroR is great to establish a baseline accuracy as any other model we develop and test has to have an accuracy better than that provided by the ZeroR. Since the ZeroR does not look at any attributes/predictors and only looks at the Class label/Target, any other model should have a better accuracy to be identified as a valid model.3. Alcohol is the attribute that has the most influence on the quality of the wine and we see that if the Alcohol value is greater than 12, the quality (Target) is good. Yes this agrees with what I noticed in question 1.4. In 10-fold cross-validation, the original sample dataset is randomly partitioned into 10 equal sized subsamples. Of the 10 subsamples, a single subsample is retained as the validation data for testing the model, and the remaining 9 subsamples are used as training data. The cross-validation process is then repeated 9 times (the folds), with each of the 10 subsamples used exactly once as the validation data. The 10 results from the folds can then be averaged to produce a single estimation. The advantage of this method over using all of the dataset to train is that all samples are used for both training and validation, and each observation is used for validation exactly once. The main reason for the difference between the two accuracies is because when we use the same exact data set to train and test, the model is built to fit the given training data set best and the test data set (same as training dataset) is not testing the generalization capability of the model and therefore the accuracy is artificially higher. Therefore cross validation is very important to test the ability of the model to generalize.5. “RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1”. Accuracy with 10 fold cross validation is 90.582%6. I tried many different classifiers and ended up chosing the RandomForest model because it provides the highest accuracy value with 10-fold cross validation amongst all the other classifiers I have seen in Weka. I tried this model with 10-fold validation on the provided training data set. I have changed a few parameters and verified the accuracy, but the default parameters for this model work best.7. I used -* Classifier A (with ten-fold validation) as “LWL -U 0 -K -1 -A "weka.core.neighboursearch.LinearNNSearch -A \"weka.core.EuclideanDistance -R first-last\"" -W weka.classifiers.trees.DecisionStump” on wine training data with an accuracy of 80.95% and on the car training data with an accuracy of 70.5% and Classifier B (with Ten fold validation) as “J48 -C 0.25 -M 2” on wine training data with an accuracy of 85.97% and on car training data with an accuracy of 89.83%. So the total accuracy difference as calculated in the PS2 is about 14%.* I chose these classifiers as I realized that I had to search for a version of the NN algorithm that did produce better results for the wine data rather than the car data. Most other classifiers almost always have better accuracy with the car data than with the wine data.8. For * f1(x), the 1-NN is the best learning function in terms of LOOCV as it produces 80% accuracy. This is because when we leave out each + or – from the given set, one by one and verify what the classifier would return when the left out data element is evaluated, we come up with correct classification 4 out of 5 times* f4(x), the 3-NN is the best learning function in terms of LOOCV as it produces 80% accuracy. This is because when we leave out each + or – from the given set, one by one and verify what the classifier would return when the left out data element is evaluated, we come up with correct classification 4 out of 5 times. NOTE: Classification here will return the majority vote amongst the 3 NN.* f2(x), Linear regression produces a fairly accurate learning function with LOOCV, as when we observe the relation between x and f2(x), we can easily see that the function can be approximated by f2(x) ~ 3x, which is a linear function. * f3(x), Polynomial regression, as when we observe the relation between x and f3(x), we see that f3(x) has to be a quadratic function (polynomial).  f3(x) ~ x**2 + 0.5, is a fairly close enough learning function that approximates the given data and performs one with LOOCV.